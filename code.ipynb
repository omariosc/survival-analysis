{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Analysis with Neural Networks\n",
    "\n",
    "The provided dataset [`MG.sav`](MG.sav) contains the survival data of `1,802` patients with _chronic heart failure_.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "For an analysis done as part of his PhD thesis, John Mbotwa reduced the this dataset to `PatientID` with `6` covariates: `Diabetes`, `StatusDeath`, `TimeDeath`, `ClinicDeath`, `Haemoglobin` and `Sex` `(de Kamps, 2023)`. The full dataset was presented in a `.sav` file called `\"MG HF cohorts 1 to 3 May 2016 censored.sav\"`, here renamed to `MG.sav` because of the white space in the title. See `Appendix B` in the report for the data dictionary.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Letus begin by loading the data and preparing it for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install pyreadstat\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set random seed for future reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "# Define path to the dataset\n",
    "mg = 'MG.sav'\n",
    "\n",
    "# Read the dataset into a Pandas dataframe\n",
    "df = pd.read_spss('MG.sav')\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Overview\n",
    "\n",
    "The `FinalData` spreadsheet lists `1802` rows, disregarding the data description line, with the `PatientId` ranging from `3` to `2540` `(de Kamps, 2023)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariates and Important Features\n",
    "\n",
    "We are interested in the event `StatusDeath`, the time `TimeDeath` and the covariates as reported by `Mbotwa et al., 2021` (i.e. `Haemoglobin`, `Diabetes`, `ClinicAge` and `MaleSex`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create a pandas dataframe containing variables of interest\n",
    "chf = df[['StatusDeath', 'TimeDeath', 'Haemoglobin', 'Diabetes', 'ClinicAge', 'MaleSex']]\n",
    "\n",
    "chf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that `MaleSex` is categorical and we must encode this. We can use the `OneHotEncoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install scikit-survival\n",
    "\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "chf = OneHotEncoder().fit_transform(chf)\n",
    "chf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "As seen before, there are missing values in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Compute number of missing values per column\n",
    "missing_values = chf.isnull().sum()\n",
    "\n",
    "# Display the dataframe\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very few. Let us quickly impute these with the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf = chf.fillna(chf.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabality Distribution\n",
    "\n",
    "Let us model the probability of survival over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "timedeath = chf['TimeDeath']\n",
    "timedeath[chf['StatusDeath'] < 1.]\n",
    "\n",
    "statusdeath = [x > 0. for x in chf['StatusDeath']]\n",
    "time, survival_prob = kaplan_meier_estimator(statusdeath, timedeath)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.step(time, survival_prob, where=\"post\", color=\"black\")\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$ (in days)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sksurv`\n",
    "\n",
    "Let's bring the event data into the stuctured array that `sksurv` expects. `(de Kamps, 2023)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "import numpy as np\n",
    "\n",
    "# Random seed for the future\n",
    "np.random.seed(seed)\n",
    "\n",
    "dfy = np.array(list(zip(statusdeath, timedeath)), dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "print(dfy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit a `CoxPHSurvivalAnalysis` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "# We do not require the StatusDeath or TimeDeath columns for `sksurv` so let us create a new dataframe without them\n",
    "dfx = chf.drop(['StatusDeath', 'TimeDeath'], axis=1, inplace=False)\n",
    "\n",
    "estimator = CoxPHSurvivalAnalysis()\n",
    "estimator.fit(dfx, dfy)\n",
    "estimator.score(dfx, dfy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the coefficients for each covariate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(estimator.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other models from `sksurv` too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "\n",
    "estimatorlasso = CoxnetSurvivalAnalysis(l1_ratio=0.99, fit_baseline_model=True)\n",
    "estimatorlasso.fit(dfx, dfy)\n",
    "estimatorlasso.score(dfx, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "\n",
    "estimatorsvm = FastSurvivalSVM(max_iter=1000, optimizer=\"rbtree\", tol=1e-5, random_state=seed)\n",
    "estimatorsvm.fit(dfx, dfy)\n",
    "estimatorsvm.score(dfx, dfy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Conclusion\n",
    "\n",
    "`SVM`, `CPH` and `CPH` with `Lasso` all seem to give similar results. We still have to do _cross validation_. `(de Kamps, 2023)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator, dfx, dfy, cv=10)\n",
    "scoreslasso = cross_val_score(estimatorlasso, dfx, dfy, cv=10)\n",
    "scoresvm = cross_val_score(estimatorsvm, dfx, dfy, cv=10)\n",
    "\n",
    "# Compare results\n",
    "print(f\"Mean score CoxPH: {scores.mean():.3f} (std: {scores.std():.3f})\")\n",
    "print(f\"Mean score Lasso: {scoreslasso.mean():.3f} (std: {scoreslasso.std():.3f})\")\n",
    "print(f\"Mean score SVM: {scoresvm.mean():.3f} (std: {scoresvm.std():.3f})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar results.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the partial likelihood as a loss function\n",
    "\n",
    "This is dual purpose. We can do a standard CPH regresssion and see if we agree on the result, but as a loss function it is also the output layer of `CoxNNet`, so if it works, it shouldn't be hard to implement that network. `(de Kamps, 2023)`\n",
    "\n",
    "The log partial likelihood is given by:\n",
    "\n",
    "$$\n",
    "\\ln L_{\\boldsymbol{\\beta}} = \\sum^N_{i=1} c_i( \\theta_i - \\ln \\sum_{j \\in R(t_j)} \\exp \\theta_j)\n",
    "$$\n",
    "\n",
    "When we allow for tied events, the procedure needs to be adapted. There are two main approaches, one by `Breslow` and one by `Efron`. Most survival analysis packages include at least these two methods. We will implement the `Breslow` one to ensure we understand the procedure, but will use `sksurv`'s implementation in the loss function.\n",
    "\n",
    "If $s_j=\\sum_{i  \\in D_j} \\boldsymbol{x}_i$ is the sum of covariates over the set $D_j$ of individuals who die at time $t_j$, then\n",
    "\n",
    "$$\n",
    "L_{\\boldsymbol{\\beta}} = \\frac{\\Pi^J_{j=1} \\exp \\boldsymbol{\\beta} \\boldsymbol{s}_j}{ \\left\\{ \\sum_{k \\in R_j} \\exp \\boldsymbol{\\beta} \\boldsymbol{x}_k   \\right\\}^{d_j}}\n",
    "$$\n",
    "\n",
    "If we look at the file `coxph.py` we see that `CoxPHOptimizer` implements this likelihood.\n",
    "\n",
    "**Note**: We have adapted this function instead of duplicating it further down to take in a `model` if specified by `CoxNNet`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Adapted from Pölsterl (2023); de Kamps (2023)\n",
    "\n",
    "class CoxPHOptimizer:\n",
    "    \"\"\"Negative partial log-likelihood of Cox proportional hazards model.\"\"\"\n",
    "\n",
    "    def __init__(self, X, event, time, alpha, ties):\n",
    "        \"\"\"Initialize the model.\n",
    "\n",
    "        Args:\n",
    "            X (pandas.DataFrame): The dataset.\n",
    "            event (array): List of events.\n",
    "            time (array): List of times.\n",
    "            alpha (array): List of L2 penalties for each coefficient.\n",
    "            ties (string): Which method to use for ties. One of 'breslow' or 'efron'.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If ties is not one of 'breslow' or 'efron'.\n",
    "        \"\"\"\n",
    "        # Sort descending\n",
    "        o = np.argsort(-time, kind=\"mergesort\")\n",
    "        self.x = torch.tensor(X[o, :], dtype=torch.float)\n",
    "        self.event = event[o]\n",
    "        self.time = time[o]\n",
    "        self.alpha = alpha\n",
    "        # Method to handle ties\n",
    "        if ties not in (\"breslow\", \"efron\"):\n",
    "            raise ValueError(\"ties must be one of 'breslow', 'efron'\")\n",
    "        ties = \"breslow\"\n",
    "        self._is_breslow = ties == \"breslow\"\n",
    "\n",
    "    def nlog_likelihood(self, w=None, model=None):\n",
    "        \"\"\"Compute negative partial log-likelihood.\n",
    "\n",
    "        Args:\n",
    "            w (array, shape = (n_features,), optional): Estimate of coefficients. Defaults to None.\n",
    "            model (torch.nn.Module, optional): The NN model. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            float: Average negative partial log-likelihood (NPLL per event).\n",
    "        \"\"\"\n",
    "        time = self.time\n",
    "        n_samples = self.x.shape[0]\n",
    "        breslow = self._is_breslow\n",
    "        xw = []\n",
    "                \n",
    "        if model is not None:\n",
    "            for i in range(n_samples):\n",
    "                # Computes risk score for each sample using the NN model\n",
    "                # and appends it to the list xw, containing the risk scores for all samples.\n",
    "                xw.append(model.forward(self.x[i]))\n",
    "                # self.update(model)\n",
    "        else:\n",
    "            # This is the critical line of code for our project. \n",
    "            # It is the matrix-vector product of the matrix of features x\n",
    "            # and the vector of weights w.\n",
    "            # This is what we will be replacing with our NNs.\n",
    "            # As seen above, when the model is not None, we use the NN model.\n",
    "            xw = self.x@w\n",
    "\n",
    "        loss = 0\n",
    "        risk_set = 0\n",
    "        k = 0\n",
    "        \n",
    "        while k < n_samples:\n",
    "            ti = time[k]\n",
    "            numerator = 0\n",
    "            n_events = 0\n",
    "            risk_set2 = 0\n",
    "            while k < n_samples and ti == time[k]:\n",
    "                if self.event[k]:\n",
    "                    numerator = numerator + xw[k]\n",
    "                    risk_set2 = risk_set2 + torch.exp(xw[k])\n",
    "                    n_events = n_events + 1\n",
    "                else:\n",
    "                    risk_set = risk_set + torch.exp(xw[k])\n",
    "                k = k + 1\n",
    "\n",
    "            if n_events > 0:\n",
    "                if breslow:\n",
    "                    risk_set = risk_set + risk_set2\n",
    "                    loss = loss - (numerator - n_events * torch.log(risk_set)) / n_samples\n",
    "                else:\n",
    "                    numerator = numerator/n_events\n",
    "                    for _ in range(n_events):\n",
    "                        risk_set = risk_set + risk_set2 / n_events\n",
    "                        loss = loss - (numerator - torch.log(risk_set)) / n_samples\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # We adapted this entire function to use torch tensors instead of numpy arrays.\n",
    "    def update(self, model, offset=0):\n",
    "        \"\"\"Compute and updates gradient and Hessian matrix.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The NN model.\n",
    "            offset (float, optional): The offset. Defaults to 0.\n",
    "        \"\"\"\n",
    "        time = self.time\n",
    "        x = self.x\n",
    "        breslow = self._is_breslow\n",
    "        n_samples, n_features = x.shape\n",
    "        \n",
    "        # Compute risk score for each sample using the NN model\n",
    "        # Everything else in this function is the same as the original code\n",
    "        exp_xw = []\n",
    "        for i in range(n_samples):\n",
    "            exp_xw.append(torch.exp(offset + model.forward(torch.tensor(x[i], dtype=torch.float32))).item())\n",
    "\n",
    "        gradient = torch.zeros((1, n_features), dtype=torch.float64)\n",
    "        hessian = torch.zeros((n_features, n_features), dtype=torch.float64)\n",
    "\n",
    "        inv_n_samples = 1. / n_samples\n",
    "        risk_set = 0\n",
    "        risk_set_x = torch.zeros((1, n_features), dtype=torch.float64)\n",
    "        risk_set_xx = torch.zeros((n_features, n_features), dtype=torch.float64)\n",
    "        k = 0\n",
    "        \n",
    "        # Iterate time in descending order\n",
    "        while k < n_samples:\n",
    "            ti = time[k]\n",
    "            n_events = 0\n",
    "            numerator = 0\n",
    "            risk_set2 = 0\n",
    "            risk_set_x2 = torch.zeros_like(risk_set_x)\n",
    "            risk_set_xx2 = torch.zeros_like(risk_set_xx)\n",
    "            while k < n_samples and ti == time[k]:\n",
    "                # Preserve 2D shape of row vector\n",
    "                xk = x[k:k + 1]\n",
    "\n",
    "                # Outer product\n",
    "                xx = torch.matmul(xk.T, xk)\n",
    "\n",
    "                if self.event[k]:\n",
    "                    numerator += xk\n",
    "                    risk_set2 += exp_xw[k]\n",
    "                    risk_set_x2 += exp_xw[k] * xk\n",
    "                    risk_set_xx2 += exp_xw[k] * xx\n",
    "                    n_events += 1\n",
    "                else:\n",
    "                    risk_set += exp_xw[k]\n",
    "                    risk_set_x += exp_xw[k] * xk\n",
    "                    risk_set_xx += exp_xw[k] * xx\n",
    "                k += 1\n",
    "\n",
    "            if n_events > 0:\n",
    "                if breslow:\n",
    "                    risk_set += risk_set2\n",
    "                    risk_set_x += risk_set_x2\n",
    "                    risk_set_xx += risk_set_xx2\n",
    "\n",
    "                    z = risk_set_x / risk_set\n",
    "                    gradient -= (numerator - n_events * z) * inv_n_samples\n",
    "\n",
    "                    a = risk_set_xx / risk_set\n",
    "                    # outer product\n",
    "                    b = torch.matmul(z.T, z)\n",
    "\n",
    "                    hessian += n_events * (a - b) * inv_n_samples\n",
    "                else:\n",
    "                    numerator /= n_events\n",
    "                    for _ in range(n_events):\n",
    "                        risk_set += risk_set2 / n_events\n",
    "                        risk_set_x += risk_set_x2 / n_events\n",
    "                        risk_set_xx += risk_set_xx2 / n_events\n",
    "\n",
    "                        z = risk_set_x / risk_set\n",
    "                        gradient -= (numerator - z) * inv_n_samples\n",
    "\n",
    "                        a = risk_set_xx / risk_set\n",
    "                        # outer product\n",
    "                        b = torch.matmul(z.T, z)\n",
    "\n",
    "                        hessian += (a - b) * inv_n_samples\n",
    "\n",
    "        self.gradient = gradient.ravel()\n",
    "        self.hessian = hessian\n",
    "    \n",
    "    def zero(self):\n",
    "        \"\"\"Reset gradient and Hessian matrix.\"\"\"\n",
    "        self.gradient = 0\n",
    "        self.hessian = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "\n",
    "An independent check on the numbers would be handy. [John Fox introductory course on survival analysis](https://socialsciences.mcmaster.ca/jfox/Courses/soc761/survival-analysis.pdf) provides a numerical example. The advantage of that example is that we get to see which coefficient belongs to what covariate. `(de Kamps, 2023)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install lifelines\n",
    "\n",
    "# (de Kamps, 2023)\n",
    "from lifelines.datasets import load_rossi\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "rossi = load_rossi()\n",
    "\n",
    "rossi_cph = CoxPHFitter().fit(rossi, 'week', 'arrest')\n",
    "\n",
    "rossi_cph.print_summary(columns=[\"coef\", \"se(coef)\", \"p\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that was a bit quick. What happened here? `lifelines` is another survival package. It was just one line to do CPH fit and the coefficients are almost identical to the example given by Fox (slide `76`). `(de Kamps, 2023)`\n",
    "\n",
    "At the very least, we should redo the analysis with `sksurv`. `rossi` is a dataframe that is understood by `Pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "rossi_events = [True if x > 0. else False for x in rossi['arrest']]\n",
    "rossi_times = [x for x in rossi['week']]\n",
    "\n",
    "rdy = np.array(list(zip(rossi_events, rossi_times)), dtype=[('Status', '?'), ('Survival_in_weeks', '<f8')])\n",
    "rdx = rossi[['fin','age','race','wexp','mar','paro','prio']]\n",
    "rdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "estimator_rossi = CoxPHSurvivalAnalysis()\n",
    "estimator_rossi.fit(rdx, rdy)\n",
    "estimator_rossi_score = estimator_rossi.score(rdx, rdy)\n",
    "print('Estimator score is:', estimator_rossi_score)\n",
    "print('Coefficients:', estimator_rossi.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, very similar results to Fox's table and importantly, the order of the coefficients is given by the order of the covariates. `(de Kamps, 2023)`\n",
    "\n",
    "Now let's go back for to the CHF data for a bit and see whether the lifelines analysis produces the same coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# (de Kamps, 2023)\n",
    "cph_chf = CoxPHFitter().fit(chf, 'TimeDeath', 'StatusDeath')\n",
    "cph_chf.print_summary(columns=[\"coef\", \"se(coef)\", \"p\"])\n",
    "\n",
    "# Store the coefficients\n",
    "cph_chf_coef = cph_chf.summary['coef']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are almost the same coefficients, and in the same order. `(de Kamps, 2023)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Do the same for the CoxPHSurvivalAnalysis on CHF\n",
    "estimator_chf = CoxPHSurvivalAnalysis()\n",
    "estimator_chf.fit(dfx, dfy)\n",
    "estimator_chf_score = estimator_chf.score(dfx, dfy)\n",
    "print('Estimator score is:', estimator_chf_score)\n",
    "print('Coefficients:', estimator_chf.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation of Partial Regression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have a free floating loss function, as yet not really tested. It's not even a loss function as such, more a methodology. Let's tidy up. `(de Kamps, 2023)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Adapted from de Kamps (2023); Ching et al. (2018)\n",
    "# We allow pasing in a model when computing the log-likelihood\n",
    "\n",
    "torch.autograd.set_detect_anomaly = True\n",
    "\n",
    "class CoxNNet(torch.nn.Module):\n",
    "    \"\"\"CoxNNet model for survival analysis.\"\"\"\n",
    "\n",
    "    def __init__(self, df, event, time, coefficients=None):\n",
    "        \"\"\"Initialize the CoxNNet model.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "            event (str): Name of the event column. \n",
    "            time (str): Name of the time column.\n",
    "            coefficients (torch.Tensor, optional): Initial coefficients. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.covariates = [x for x in df.columns if x != event and x != time]\n",
    "        self.df = df\n",
    "        self.dfx = df[self.covariates].to_numpy(dtype=float)\n",
    "        self.events = df[event].to_numpy(dtype=float)\n",
    "        self.times = df[time].to_numpy(dtype=float)\n",
    "        \n",
    "        n_covariates = len(self.covariates)\n",
    "        \n",
    "        # If no coefficients are passed in, we initialize them randomly\n",
    "        if coefficients == None:\n",
    "            # Set random seed for reproducibility\n",
    "            torch.manual_seed(42)\n",
    "            self.coefficients = torch.randn(n_covariates, requires_grad=True, dtype=torch.float)\n",
    "        # Otherwise we use the coefficients passed in\n",
    "        else:\n",
    "            self.coefficients = coefficients.clone().detach().requires_grad_(True)\n",
    "        # print(self.coefficients)\n",
    "        \n",
    "        # This line is used in coxpy to initialize the alphas\n",
    "        alphas = torch.zeros(self.dfx.shape[1])\n",
    "        \n",
    "        self.opt = CoxPHOptimizer(self.dfx, self.events, self.times, alphas, 'breslow')\n",
    "\n",
    "    def forward(self, model=None):\n",
    "        \"\"\"Calculate the log-likelihood over the entire dataset.\n",
    "        This is the function that is called when we call net.forward().\n",
    "        Important: this is where we can pass in a NN model.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module, optional): NN model to use. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            float: Log-likelihood over the entire dataset.\n",
    "        \"\"\"\n",
    "        # sksurv calculates the llh /event and puts a minus sign in front, we undo that\n",
    "        total = -self.df.shape[0]*self.opt.nlog_likelihood(self.coefficients, model)\n",
    "        return total\n",
    "\n",
    "# Simply testing to make sure the code works\n",
    "c = torch.tensor([-0.38, -0.06, 0.31, -0.15, -0.43, -0.08, 0.09])\n",
    "net = CoxNNet(rossi, 'arrest', 'week', coefficients=c)\n",
    "loss = net.forward()\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly identical to `lifelines` estimate. `(de Kamps, 2023)`\n",
    "\n",
    "We can do the same for the `CHF` data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_c = torch.tensor([0.04, -0.20, 0.30, 0.57], requires_grad=True)\n",
    "chf_net_c = CoxNNet(chf, 'StatusDeath', 'TimeDeath', coefficients=chf_c)\n",
    "chf_net_c.forward().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "We will use the `autograd` functionality provided by `PyTorch` to do the differentiation to maximise the partial log-likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Adapted from de Kamps (2023)\n",
    "def gradient_descent(df, event, time, lr, n_epoch, coefficients=None):\n",
    "    \"\"\"Performs gradient descent on the Cox model.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        lr (float): Learning rate.\n",
    "        n_epoch (int): Number of epochs (iterations to run).\n",
    "        coefficients (torch.Tensor, optional): Initial coefficients. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained CoxNNet model (with updated coefficients).\n",
    "    \"\"\"\n",
    "    \n",
    "    net = CoxNNet(df, event, time, coefficients)\n",
    "    loss = net.forward()\n",
    "    print(\"Initial Loss:\", loss.item())\n",
    "    \n",
    "    # Iterate n_epoch times\n",
    "    for _ in range(n_epoch):\n",
    "        # Updates the coefficients using autograd\n",
    "        loss = net.forward()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            # Use the specified learning rate\n",
    "            net.coefficients += net.coefficients.grad * lr\n",
    "            net.coefficients.grad.zero_()\n",
    "    loss = net.forward()\n",
    "    \n",
    "    # Print the final loss and the updated coefficients\n",
    "    print(\"Final Loss:\", loss.item())\n",
    "    print(\"Coefficients:\", net.coefficients.detach().numpy())\n",
    "    return net\n",
    "\n",
    "rossi_net = gradient_descent(rossi, 'arrest', 'week', 0.0001, 4000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values found by `CoxNNet` for the coefficients appear close to the `lifelines` results, it appears we have successfully used `PyTorch` to calculate a gradient for the loss function and perform _steepest gradient descent_. Let's test on the `CHF` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Note: this takes a while to run\n",
    "# We tried to use the same learning rate (i.e. 0.0001) but that did not work\n",
    "# Thus, we used a smaller learning rate (i.e. 0.000001)\n",
    "chf_net = gradient_descent(chf, 'StatusDeath', 'TimeDeath', 0.000001, 4000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, same results. We should probably also create a gradient descent method for training NN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Train the NN using gradient descent\n",
    "def train_model(df, event, time, model, lr, n_epoch, output=False):\n",
    "    \"\"\"Trains a NN model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        model (_type_): NN model.\n",
    "        lr (float): Learning rate.\n",
    "        n_epoch (int): Number of epochs (iterations to run).\n",
    "        output (bool, optional): Whether to print the loss before and after training. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        float: The final loss.\n",
    "    \"\"\"\n",
    "    net = CoxNNet(df, event, time)\n",
    "    losses = []\n",
    "    \n",
    "    # Print the initial loss before training\n",
    "    if output:\n",
    "        print(\"Initial Loss:\", net.forward(model).item())\n",
    "    \n",
    "    # Train the model\n",
    "    for _ in range(n_epoch):\n",
    "        # Compute the loss using the CoxNNet class and the NN model\n",
    "        loss = net.forward(model)\n",
    "        loss.backward()        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Update the weights using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param += param.grad * lr\n",
    "            model.zero_grad()\n",
    "    \n",
    "    # Print the final loss\n",
    "    if output:\n",
    "        print(\"Final Loss:\", net.forward(model).item())\n",
    "        \n",
    "        # Produce a plot of the loss over the epochs\n",
    "        losses = [x.detach().numpy() for x in losses]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(range(1, n_epoch+1), losses, color='black')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Negative Partial Log-Likelihood Loss')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return the final PLE loss of the final trained model\n",
    "    return losses[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this later. For now, let's use a way to measure the performance of the model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance Statistics\n",
    "\n",
    "The _concordance statistic_ (also known as the c-statistic) is a measure of the goodness of fit of a survival model. It quantifies the model's ability to correctly order the predicted survival times of pairs of individuals. A higher c-statistic indicates a better model fit. Here's how to compute the c-statistic given a set of survival times and a set of predicted survival times:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def concordance_statistic(risks, events, times, output=True):\n",
    "    \"\"\"Calculates the c-statistic for a given set of predicted risks and actual events and times.\n",
    "\n",
    "    Args:\n",
    "        risks (array): List of predicted risks.\n",
    "        events (array): List of event indicators.\n",
    "        times (array): List of survival times.\n",
    "\n",
    "    Returns:\n",
    "        float: c-statistic.\n",
    "    \"\"\"\n",
    "    valid_pairs = 0\n",
    "    concordant_pairs = 0\n",
    "    discordant_pairs = 0\n",
    "    tied_pairs = 0\n",
    "    \n",
    "    # Convert risks tensor to NumPy array\n",
    "    if isinstance(risks, torch.Tensor):\n",
    "        risks = risks.detach().cpu().numpy()\n",
    "    \n",
    "    # Sort risks, events, and times based on the times\n",
    "    sorted_indices = np.argsort(times)\n",
    "    risks = np.array(risks)[sorted_indices]\n",
    "    events = np.array(events)[sorted_indices]\n",
    "    times = np.array(times)[sorted_indices]\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            if times[i] != times[j]:\n",
    "                if events[i] and (times[i] < times[j]):\n",
    "                    valid_pairs += 1\n",
    "                    if risks[i] > risks[j]:\n",
    "                        concordant_pairs += 1\n",
    "                    elif risks[i] < risks[j]:\n",
    "                        discordant_pairs += 1\n",
    "                    else:\n",
    "                        tied_pairs += 1\n",
    "\n",
    "                elif events[j] and (times[j] < times[i]):\n",
    "                    valid_pairs += 1\n",
    "                    if risks[j] > risks[i]:\n",
    "                        concordant_pairs += 1\n",
    "                    elif risks[j] < risks[i]:\n",
    "                        discordant_pairs += 1\n",
    "                    else:\n",
    "                        tied_pairs += 1\n",
    "    \n",
    "    # We can optionally produce a plot of the risk scores over time\n",
    "    # This is useful for visualising the risk scores\n",
    "    # Ideally, we would like to see the risk scores of the events (i.e. 1) higher than the non-events (i.e. 0)\n",
    "    # This is because the risk scores are the log-hazard ratios\n",
    "    if output:\n",
    "        # Plots the risk scores over time\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        colors = np.where(events, 'orange', 'blue')\n",
    "\n",
    "        for color, label in zip(['orange', 'blue'], ['Event', 'Non-Event']):\n",
    "            mask = (colors == color)\n",
    "            plt.scatter(times[mask], np.array(risks)[mask], alpha=0.3, color=color, label=label)\n",
    "\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Risk')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Note we do not actually need to use discordant_pairs in the calculation\n",
    "    # Since we already increment valid_pairs when we encounter a discordant pair\n",
    "    return (concordant_pairs + 0.5 * tied_pairs) / valid_pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `concordance_statistic` calculates the c-statistic given the predicted risks, event indicators, and survival times. It iterates through all valid pairs of individuals and counts the number of concordant, discordant, and tied pairs. The c-statistic is then calculated as the ratio of the sum of concordant pairs and half the tied pairs to the total number of valid pairs.\n",
    "\n",
    "After defining the function, the predicted risks for each individual in the dataset are calculated by multiplying the covariate matrix `net.dfx` with the optimized model coefficients. The c-statistic is then computed using the `concordance_statistic` function, and the result is printed.\n",
    "\n",
    "We can define another function `get_concordance` which takes in a `CoxNNet` model, a dataframe, an event and a time, and returns the c-statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def get_concordance_statistic(df, event, time, net, output=True):\n",
    "    \"\"\"Calculates the c-statistic for a given dataset for the CoxNNet model.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        net (torch.nn.Module): CoxNNet model.\n",
    "\n",
    "    Returns:\n",
    "        float: c-statistic.\n",
    "    \"\"\"\n",
    "    # Calculate the predicted risk for each individual in the dataset\n",
    "    risks = np.matmul(net.dfx, net.coefficients.detach().numpy())\n",
    "    \n",
    "    return concordance_statistic(risks, df[event].to_numpy(), df[time].to_numpy(), output)\n",
    "\n",
    "print(\"Base Rossi Score:\", get_concordance_statistic(rossi, 'arrest', 'week', rossi_net))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same c-statistic as the one used by `lifelines` for the `Rossi` dataset. We can see that these coefficients may be the best, but they do not have high predictive acuity. Ideally, we see clusters of points for each class - each class should be well separated. Since this would be difficult to do in practice, our aim is that those who survived less have a higher risk than those who survived longer. This is not the case here, as we can see the risk values completely ranged at all values of times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(\"Base CHF Score:\", get_concordance_statistic(chf, 'StatusDeath', 'TimeDeath', chf_net))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance Statistic for Neural Network Models\n",
    "\n",
    "Now, let us adapt the `get_concordance_statistic` to allow for a NN model. We need to compute the risk for an invidual using the NN model, instead of computing the dot product using the coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Calculate the predicted risk for each individual in the dataset\n",
    "def get_concordance_statistic_nn(df, event, time, model, output=True):\n",
    "    \"\"\"Calculates the c-statistic for a given dataset using a NN model to compute predicted risks.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the covariates and the event and time columns.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        model (torch.nn.Module): NN model.\n",
    "        output (bool, optional): Whether to print the loss before and after training. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        float: c-statistic.\n",
    "    \"\"\"\n",
    "    # Calculate the predicted risk for each individual in the dataset using the NN\n",
    "    x = torch.tensor(df[[col for col in df.columns if col not in [event, time]]].values, dtype=torch.float)    \n",
    "    with torch.no_grad():\n",
    "        predicted_risk = np.array([model.forward(xi).item() for xi in x])\n",
    "    \n",
    "    # Compute the c-statistic\n",
    "    return concordance_statistic(predicted_risk, df[event].to_numpy(), df[time].to_numpy(), output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test this on a simple NN which is essentially the same as the CPH model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# This NN essentially computes the same as the CoxPH model\n",
    "# i.e. xw = self.x@w = x1w1 + x2w2 + x3w3 + x4w4 + ... + xnw_n\n",
    "# This is computed in the forward pass\n",
    "# We call it linear because it is a linear model, i.e. it is a linear combination of the covariates\n",
    "# and does not have any hidden layers, thus not capturing any non-linear relationships.\n",
    "class FixedWeightLinear(torch.nn.Module):\n",
    "    \"\"\"Simple NN with no hidden layers that simply computes the dot product of the input and the weights.\n",
    "    This is essentially the same as the CoxPH model.\n",
    "    We do not train this model, we simply use it to compute the predicted risk for each individual in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        \"\"\"Initializes the FWL model.\n",
    "\n",
    "        Args:\n",
    "            weights (torch.Tensor): Weights to use for the dot product.\n",
    "        \"\"\"\n",
    "        super(FixedWeightLinear, self).__init__()\n",
    "        # Initialize the weights as a torch.nn.Parameter\n",
    "        self.weights = torch.nn.Parameter(weights.clone().detach().requires_grad_(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Simply multiply the weights by the input.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        return torch.matmul(x, self.weights)\n",
    "    \n",
    "# Initialize the FWL model with the learned coefficients\n",
    "rossi_fwl = FixedWeightLinear(rossi_net.coefficients)\n",
    "\n",
    "# Compute the c-statistic using the FWL model\n",
    "print(\"FWL Rossi Score:\", get_concordance_statistic_nn(rossi, 'arrest', 'week', rossi_fwl))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieeve the same c-statistic. As we have seen we have been able to reproduce coefficients and the c-statistic scores, we have everything needed to begin building NNs.\n",
    "\n",
    "## Neural Network Exploration\n",
    "\n",
    "First, we will begin by adapting the `FixedWeightLinear` model so that we may use random weights. We will need to be able to train the model with random weights, and then use the optimizer to find the best weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# This class lets us use random weights for each feature\n",
    "class VariableWeightLinear(FixedWeightLinear):\n",
    "    \"\"\"Simple NN with no hidden layers that simply computes the dot product of the input and the weights.\n",
    "    However, the weights are randomly initialized and then trained using gradient descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        \"\"\"Initializes the VWL model.\n",
    "\n",
    "        Args:\n",
    "            n (int): Number of features in the dataset.\n",
    "        \"\"\"\n",
    "        # Randomise weights\n",
    "        torch.manual_seed(42)\n",
    "        weights = torch.nn.Parameter(torch.randn(n, requires_grad=True, dtype=torch.float))\n",
    "        super(VariableWeightLinear, self).__init__(weights)\n",
    "        # Print these initial weights\n",
    "        print(\"Initial VWL\", self.weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Simply multiply the weights by the input.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        # Again, simply multiply the weights by the input\n",
    "        return torch.matmul(x, self.weights)\n",
    "    \n",
    "# Initialize the VariableWeightLinear model\n",
    "n_covariates = len([x for x in rossi.columns if x != 'arrest' and x != 'week'])\n",
    "rossi_vwl = VariableWeightLinear(n_covariates)\n",
    "\n",
    "# Compute the c-statistic using the trained VWL model\n",
    "print(\"Initial VWL Rossi Score:\", get_concordance_statistic_nn(rossi, 'arrest', 'week', rossi_vwl, False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train this model. The goal is to produce the same weights as the CPH model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Train the model using the train_model function we created before\n",
    "rossi_vwl_loss = train_model(rossi, 'arrest', 'week', rossi_vwl, 0.0001, 4000, False)\n",
    "\n",
    "# Compute the c-statistic using the trained VWL model\n",
    "rossi_vwl_c = get_concordance_statistic_nn(rossi, 'arrest', 'week', rossi_vwl, False)\n",
    "\n",
    "# Print the trained weights and loss in format string\n",
    "print(\"Trained VWL Rossi Weights: {0} \\\n",
    "      \\nTrained VWL Rossi Loss: {1} \\\n",
    "      \\nTrained VWL Rossi Score {2}\"\n",
    "      .format(rossi_vwl.weights, rossi_vwl_loss, rossi_vwl_c))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weights are very close to the CPH model. Now we can begin experimenting with different NN architectures.\n",
    "\n",
    "### Single Hidden Layer\n",
    "\n",
    "This simple NN class allows us to experiment with the number of neurons in a single hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def plot_weights(weights):\n",
    "    \"\"\"Plots the distribution of the weights.\n",
    "\n",
    "    Args:\n",
    "        weights (torch.Tensor): Weights to plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(weights, bins=int(np.sqrt(len(weights))), color='black')\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "class SNN(torch.nn.Module):\n",
    "    \"\"\"Simple NN with one hidden layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=None):\n",
    "        \"\"\"Initializes the SNN model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of covariates.\n",
    "            hidden_size (int): Number of neurons in the hidden layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Randomise weights\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        super(SNN, self).__init__()\n",
    "        # Store the input and hidden sizes\n",
    "        self.input_size = input_size\n",
    "        # If no hidden size is specified, use the sqrt(input_size + 1) rounded up\n",
    "        if hidden_size is None:\n",
    "            self.hidden_size = np.sqrt(input_size + 1).astype(int) + 1\n",
    "        else:\n",
    "            self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create the layers\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = torch.nn.Sequential(self.fc1, self.relu, self.fc2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predicts the risk for a single individual.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def return_weights(self):\n",
    "        \"\"\"Plots the weights in a histogram.\"\"\"\n",
    "        \n",
    "        plot_weights(torch.cat([self.fc1.weight.view(-1), self.fc1.bias, self.fc2.weight.view(-1), self.fc2.bias]).detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a class which we can use in the future to help test this model, and other models in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# This allows us to test different architectures of the `NN` model\n",
    "def test_nn(df, event, time, model, lr, n_epoch, output=False):\n",
    "    \"\"\"Trains the model and prints the c-statistic.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataset to use.\n",
    "        event (str): Name of the event column.\n",
    "        time (str): Name of the time column.\n",
    "        model (torch.nn.Module): NN model.\n",
    "        lr (float): Learning rate.\n",
    "        n_epoch (int): Number of epochs.\n",
    "        output (bool, optional): Whether to print the output. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = train_model(df, event, time, model, lr, n_epoch, output)\n",
    "    score = get_concordance_statistic_nn(df, event, time, model, output)\n",
    "    \n",
    "    if output:\n",
    "        # Print format string with loss and score\n",
    "        param = \"\"\n",
    "        if type(model).__name__ == \"SNN\":\n",
    "            param = f\"[{model.input_size}, {model.hidden_size}, 1]\"\n",
    "        elif type(model).__name__ == \"GNN\": # This will be useful later\n",
    "            param = model.layer_sizes\n",
    "        print(f\"{type(model).__name__}{param} \\n- Loss: {loss}\\n- Score: {score}\")\n",
    "    return loss, score\n",
    "\n",
    "# Test the simple model on the Rossi data set\n",
    "input_size = rossi.shape[1] - 2  # Number of input features (excluding censor and time variables)\n",
    "hidden_size = (np.sqrt(input_size + 1)).astype(int) + 1  # Number of hidden units\n",
    "rossi_snn = SNN(input_size, hidden_size)\n",
    "test_nn(rossi, 'arrest', 'week', rossi_snn, 0.00001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already performed better than the previous models. Let's investigate the effect of the number of neurons in the hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create a loop to test different architectures (begin with 1 neuron in single hidden layer, increase in steps of 1 until 2 * input_size)\n",
    "rossi_snn_losses = []\n",
    "rossi_snn_scores = []\n",
    "\n",
    "# This allows us to test different architectures of the NN model\n",
    "# We do not want to add too many neurons because this can lead to overfitting\n",
    "rossi_snn_hidden_sizes = [1 + i for i in range(input_size * 2)]\n",
    "\n",
    "# This stores the best model and its score\n",
    "rossi_snn_best = [None, 0, 0]\n",
    "\n",
    "# Loop through the different architectures\n",
    "for i in rossi_snn_hidden_sizes:\n",
    "    model = SNN(input_size, i)\n",
    "    loss, score = test_nn(rossi, 'arrest', 'week', model, 0.0001, 1000, False)\n",
    "    loss = loss.detach().numpy()\n",
    "    if score > rossi_snn_best[2]:\n",
    "        rossi_snn_best = [model, loss, score]\n",
    "    rossi_snn_losses.append(loss)\n",
    "    rossi_snn_scores.append(score)\n",
    "\n",
    "# Print the scores of the best architecture\n",
    "print(\"Best SNN Rossi: \\\n",
    "    \\n- Architecture: SNN[{0}, {1}, 1] \\\n",
    "    \\n- Loss: {2} \\\n",
    "    \\n- Score: {3}\"\n",
    "    .format(input_size, rossi_snn_best[0].hidden_size, rossi_snn_best[1], rossi_snn_best[2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better performance. We could use more than `13` neurons in the hidden layer, however, we should not use more than double the number of input neurons in the hidden layer. Let us visualise the performance for the other models too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Plot the losses\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(rossi_snn_hidden_sizes, rossi_snn_losses, label='Loss', color='black')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Loss ')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the datapoint on the bottom left is the same as the regular model, simply multiplying the coefficients by the covariates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Plot the c-Score scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(rossi_snn_hidden_sizes, rossi_snn_scores, label='C-Statistic', color='black')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('C-Statistic')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. The graphs indicate that the size of the hidden layer does not really matter.\n",
    "\n",
    "Let's also visualize the weights of the model to see if we have potential overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Use a histogram with n_bins = size of data square root\n",
    "rossi_snn_best[0].return_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the weights are quite small. This is a good sign. Instead of using a single hidden layer, let's try using multiple.\n",
    "\n",
    "### Multiple Hidden Layers\n",
    "\n",
    "Let us create a more general NN class that allows us to experiment with the number of hidden layers and neurons in each layer.\n",
    "\n",
    "The goal is to see how the number of parameters affects the performance of the NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"General NN with any number of hidden layers with any number of neurons.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Initializes the GNN model.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes (array): List of the number of neurons in each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Randomise weights\n",
    "        torch.manual_seed(42)\n",
    "        super(GNN, self).__init__()\n",
    "        # We want to store the sizes in a list for the future\n",
    "        self.layer_sizes = layer_sizes\n",
    "        # Create a list of layers\n",
    "        self.layers = torch.nn.ModuleList()        \n",
    "        # Create the layers based on the input sizes\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Add a linear layer\n",
    "            self.layers.append(torch.nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            # Add a ReLU activation function after every linear layer except the last one\n",
    "            if i < len(layer_sizes) - 2:  # Don't add ReLU activation after the last linear layer\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "        # Create the model\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predicts the risk for a single individual.\n",
    "\n",
    "        Args:\n",
    "            x (array): List of covariates for a single individual.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted risk.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def return_weights(self):\n",
    "        \"\"\"Plots the weights in a histogram.\"\"\"\n",
    "        \n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if type(layer).__name__ == \"Linear\":\n",
    "                weights.append(layer.weight.view(-1))\n",
    "                weights.append(layer.bias)\n",
    "                \n",
    "        plot_weights(torch.cat(weights).detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class takes a list of integers as input, where each integer represents the number of neurons in that layer. The first integer is the input size, the last integer should be `1` (as the final output neuron should be `1`), and the integers in between represent the number of neurons in each hidden layer. We could adapt this class to allow us to further experiment with the properties of the NN.\n",
    "\n",
    "We will stick to using simple architectures that are easy to understand. These should capture the relationships of the covariates and not be too complex to overfit the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_gnn = GNN([7, 8, 3, 1]) # 7 input neurons, 2 hidden layers with 8 and 3 neurons, 1 output neuron\n",
    "test_nn(rossi, 'arrest', 'week', rossi_gnn, 0.0001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as the `SNN`. Let us investigate further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_gnn = GNN([7, 14, 3, 1]) # 7 input neurons, 2 hidden layers with 14 and 3 neurons, 1 output neuron\n",
    "test_nn(rossi, 'arrest', 'week', rossi_gnn, 0.0001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one could not produce a solution. Let us try more neurons in the final hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_architecture = [7, 14, 7, 1] # 7 input neurons, 2 hidden layers with 14 and 7 neurons, 1 output neuron\n",
    "rossi_gnn = GNN(rossi_architecture)\n",
    "test_nn(rossi, 'arrest', 'week', rossi_gnn, 0.0001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we see that increasing the number of hidden layers and neurons does not neccessarily produce a better model. This may be due to overfitting, or maybe the neural network can no longer capture any more of the data. Let us again visualise the weights of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Visualise weights of the GNN model using a histogram\n",
    "# We expect there to be exponentially more weights in the GNN model than the SNN model\n",
    "rossi_gnn.return_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is good. The weights do not seem to cause overfitting.\n",
    "\n",
    "We considered exploring other methods, including _regularisation_, the number of _epochs_, the _learning rate_, the _activation function_, the _optimizer_, _feature processing_, _ensemble methods_ and _early stopping_. However, we will leave this for another time. It is unlikely this will improve performance. It seems the models can only perform so well. Let's investigate the data further.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Class Analysis\n",
    "\n",
    "_Latent Class Analysis_ (LCA) is a statistical method for identifying hidden subgroups within a population. In the context of our problem, we can use LCA to separate patients into different subgroups and then train separate `GNN` models for each subgroup.\n",
    "\n",
    "To create a class for latent class analysis using the CPH model, we can:\n",
    "\n",
    "1. Split the dataset into a given number of components (latent classes).\n",
    "2. Train a separate `GNN` model for each component.\n",
    "3. Produce the final loss and c-statistic over all components.\n",
    "\n",
    "To split the data, we can use the `KMeans` clustering algorithm. This algorithm takes in a number of clusters and returns a list of cluster labels for each individual in the dataset. The `KMeans` algorithm is used as the default method in `GuassianMixture`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install sklearn\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "class LCA:\n",
    "    \"\"\"Latent Class Analysis model for survival analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, event, time, n_components=2, architecture=None):\n",
    "        \"\"\"Initialises the LCA model.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe containing the data.\n",
    "            event (str): Name of the event column.\n",
    "            time (str): Name of the time column.\n",
    "            n_components (int, optional): Number of components in the Gaussian Mixture Model. Defaults to 2.\n",
    "            architecture (torch.Tensor, optional): Architecture of the GNN model. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.components = n_components\n",
    "        self.df = df\n",
    "        self.event = event\n",
    "        self.time = time\n",
    "        \n",
    "        # Initialise the covariates and the sizes\n",
    "        # We take away 1 as we do not want to include the class, event and time variables\n",
    "        self.input_size = len(self.df.columns) - 3\n",
    "        \n",
    "        # If the architecture is not None, we use the architecture provided\n",
    "        if architecture is None:\n",
    "            self.architecture = [self.input_size, \n",
    "                                 self.input_size + 1, np.sqrt(self.input_size + 1).astype(int) + 1,\n",
    "                                 1]\n",
    "        else:\n",
    "            self.architecture = architecture\n",
    "        # Initialise the models\n",
    "        # We use GNNs as the models because they are more flexible than SNNs\n",
    "        gnn = GNN(self.architecture)\n",
    "        self.models = [gnn for _ in range(n_components)]\n",
    "        # Initialise the Gaussian Mixture Model\n",
    "        # We set the random state for reproducibility\n",
    "        self.gmm = GaussianMixture(n_components=n_components, random_state=seed)\n",
    "        \n",
    "        # Stores dataframe for each latent class\n",
    "        self.classes = []\n",
    "        \n",
    "        # Store risks for each each patient\n",
    "        self.risks = []\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Fits the LCA model to the dataset. and creates the seprate latent classes.\"\"\"\n",
    "        \n",
    "        # Fit Gaussian Mixture Model to the dataset using Expectation Maximisation algorithm\n",
    "        self.gmm.fit(self.df)\n",
    "        \n",
    "        # Assign patients to the latent classes\n",
    "        self.df['LatentClass'] = self.gmm.predict(self.df)\n",
    "    \n",
    "    def train(self, lr=0.00001, epochs=1000, output=False):\n",
    "        \"\"\"Trains the GNN models for each latent class.\n",
    "        \n",
    "        Args:\n",
    "            lr (float, optional): Learning rate. Defaults to 0.00001.\n",
    "            epochs (int, optional): Number of epochs. Defaults to 1000.\n",
    "            output (bool, optional): Whether to print the loss and output loss graph. Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "            float: Total loss of the model.\n",
    "        \"\"\"  \n",
    "\n",
    "        # Append the dataframes for each latent class to the classes list\n",
    "        for i in range(self.components):\n",
    "            self.classes.append(self.df.loc[self.df['LatentClass'] == i].drop('LatentClass', axis=1))\n",
    "            # Remove the latent class column from the dataframe\n",
    "            self.classes[i] = self.df.drop('LatentClass', axis=1)        \n",
    "            \n",
    "        # Initialise the total loss\n",
    "        total_loss = 0\n",
    "        \n",
    "        # For each model, train the model on the data for the given latent class\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Train the model of the latent class\n",
    "            loss = train_model(self.classes[i], self.event, self.time, model, lr, epochs, output)\n",
    "            print(\"Loss for latent class \" + str(i) + \": \" + str(loss.detach().numpy()))\n",
    "            # Add the loss to the total loss (remember the loss is negative)\n",
    "            total_loss += -loss\n",
    "        \n",
    "        return total_loss.detach().numpy()\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Produces the predictions for each patient in the dataset and computes the final loss and Score.\"\"\"\n",
    "\n",
    "        classes = []\n",
    "        # Iterate over all patients and compute the risk for each patient using the appropriate model\n",
    "        for _, x in self.df.iterrows():\n",
    "            latent_class = x['LatentClass']\n",
    "            # Get the model for the patient\n",
    "            model = self.models[int(latent_class)]\n",
    "            # Remove LatentClass, time and event column from the patient data\n",
    "            x = x.drop('LatentClass').drop(self.event).drop(self.time)\n",
    "            # Make patient a tensor\n",
    "            x = torch.tensor(x.to_numpy(), dtype=torch.float32)\n",
    "            # Compute the risk for the patient\n",
    "            with torch.no_grad():\n",
    "                risk = model.forward(x)\n",
    "            # Append the risk to the list of risks\n",
    "            self.risks.append(risk)\n",
    "            \n",
    "        # Use the predicted risks to compute the score  \n",
    "        return concordance_statistic(self.risks, self.df[self.event].to_numpy(), self.df[self.time].to_numpy(), True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation uses _Gaussian Mixture Models_ (GMM) to split the dataset into the specified number of components. We can use this class by initializing it with the desired number of components, then fit it with our data, which trains models for each latent class. We can then predict the risk for new patients and produce the overall c-statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def lca(df, event, time, n=2, architecture=None, lr=0.0001, epochs=1000, output=False):\n",
    "    \"\"\"Function to fit the LCA model and train the GNN models.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe containing the data\n",
    "        event (str): Name of the event column\n",
    "        time (str): Name of the time column\n",
    "        n (int): Number of latent classes\n",
    "        architecture (list, optional): Architecture of the GNN model. Defaults to None.\n",
    "        lr (float, optional): Learning rate. Defaults to 0.0001.\n",
    "        epochs (int, optional): Number of epochs. Defaults to 1000.\n",
    "        output (bool, optional): Whether to print the loss and output loss graph. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        LCA: Fitted LCA model.\n",
    "    \"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # Initialise LatentClass column as 0\n",
    "    X['LatentClass'] = 0\n",
    "\n",
    "    # Fits the LCA model\n",
    "    model = LCA(X, event, time, n_components=n, architecture=architecture)\n",
    "    model.fit()\n",
    "    \n",
    "    # Print the number of patients in each latent class\n",
    "    print(model.df['LatentClass'].value_counts())\n",
    "    \n",
    "    # Trains the GNN models\n",
    "    loss = model.train(lr, epochs, output)\n",
    "    \n",
    "    score = model.predict()\n",
    "    \n",
    "    print(\"Total Loss: {0} \\nScore: {1}\".format(loss, score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this class on the `Rossi` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca1 = lca(rossi, 'arrest', 'week', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have achieved the same results as before, we know we have implemented the class correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca2 = lca(rossi, 'arrest', 'week', 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better. Let us investigate the architecture of the `GNN`'s too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca2_arch = lca(rossi, 'arrest', 'week', 2, rossi_architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model so far. Now let us test this using `3` classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca3 = lca(rossi, 'arrest', 'week', 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not perform as well as the `2` class model. Let us investigate the architecture of the `GNN`'s too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca3_arch = lca(rossi, 'arrest', 'week', 3, rossi_architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the best model so far. Let's finally try using `4` classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca4 = lca(rossi, 'arrest', 'week', 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs better than the `2` and the `3` class models with the same archicture. Let's adapt the architecture here and see if it produces the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_lca4_arch = lca(rossi, 'arrest', 'week', 4, rossi_architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the best model so far. As expected by `Mbotwa et al. (2021)`, it was unlikely that NNs would have greatly increased predictive acuity. It seems that using `LCA` does lead to better results. We also see that the choice of architecture can affect the performance of the model a little. Let' try a completely different method.\n",
    "\n",
    "## Convolutional Neural Network\n",
    "\n",
    "_Convolutional Neural Networks_ (CNNs) are a type of neural network that are commonly used for image classification. They are able to capture the spatial relationships between pixels in an image. We can use CNNs to capture the spatial relationships between the covariates in our dataset. By transforming a set of patient data into an image, i.e. a grid of pixels where each pixel represents a covariate, we can then use a CNN to make predictions of a patient's risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create a simple CNN\n",
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"Simple CNN model for survival analysis.\"\"\"\n",
    "    def __init__(self, df, event, time, width=0, height=0):\n",
    "        \"\"\"Initialises the CNN model.\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe containing the data\n",
    "            event (str): Name of the event column\n",
    "            time (str): Name of the time column\n",
    "            width (int, optional): Width of the input image. Defaults to 0.\n",
    "            height (int, optional): Height of the input image. Defaults to 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Dataframe containing the data\n",
    "        self.df = df\n",
    "        \n",
    "        # Event and time columns\n",
    "        self.event = event\n",
    "        self.time = time\n",
    "                \n",
    "        # Dataframe containing covariates\n",
    "        self.dfx = self.df.drop([self.event, self.time], axis=1, inplace=False)\n",
    "        # Convert dataframe to numpy array\n",
    "        self.df_samples = self.dfx.to_numpy()\n",
    "        \n",
    "        # Calculate the optimal width and height based on the number of covariates\n",
    "        # unless passed in\n",
    "        if width == 0 or height == 0:\n",
    "            self.width, self.height = self.get_optimal_dimensions(self.dfx.shape[1])\n",
    "        else:\n",
    "            self.width = width\n",
    "            self.height = height\n",
    "        \n",
    "        # Use random seed for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Create the model\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(32 * (self.width // 2) * (self.height // 2), 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 1)\n",
    "        self.layers = [self.conv1, self.pool, self.fc1, self.fc2]\n",
    "        self.model = torch.nn.Sequential(*self.layers)\n",
    "        \n",
    "        # Transform the data\n",
    "        self.transform()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Produces risk scores for the given patient image.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Patient image\n",
    "\n",
    "        Returns:\n",
    "            float: Risk score\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * (self.width // 2) * (self.height // 2))\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def transform(self):\n",
    "        \"\"\"Transforms the data to be used by the model.\"\"\"\n",
    "        \n",
    "        # Convert features to images\n",
    "        self.images = [self.features_to_image(x, self.width, self.height) for x in self.df_samples]\n",
    "        \n",
    "        # Prepare the data for the CNN\n",
    "        self.tensors = [torch.tensor(image, dtype=torch.float32).unsqueeze(0) for image in self.images]\n",
    "        self.tensor = torch.stack(self.tensors)\n",
    "        self.labels = torch.tensor(self.df[self.event].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def train(self, lr=0.0001, epochs=1000, output=True):\n",
    "        \"\"\"Trains the model.\n",
    "        \n",
    "        Args:\n",
    "            lr (float): Learning rate\n",
    "            epochs (int): Number of epochs\n",
    "            output (bool, optional): Whether to output the loss plot. Defaults to True.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note we use a custom optimizer here as the original classes are not suitable\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        scores = []\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.forward(self.tensor)\n",
    "\n",
    "            # Calculate concordance statistic\n",
    "            score = concordance_statistic(outputs, self.df[self.event].to_numpy(), self.df[self.time].to_numpy(), output=False)\n",
    "\n",
    "            # Calculate the negative of the concordance statistic to maximize it\n",
    "            loss = -torch.tensor(score, requires_grad=True)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save score\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Plot losses\n",
    "        if output:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(scores)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "            \n",
    "    def get_optimal_dimensions(self, n_features):\n",
    "        \"\"\"Calculate the optimal width and height for a given number of features.\n",
    "        \n",
    "        Args:\n",
    "            n_features (int): Number of features\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Optimal width and height\n",
    "        \"\"\"\n",
    "        \n",
    "        sqrt_n_features = int(np.sqrt(n_features))\n",
    "        if sqrt_n_features * sqrt_n_features == n_features:\n",
    "            return sqrt_n_features, sqrt_n_features\n",
    "        else:\n",
    "            for i in range(sqrt_n_features + 1, n_features + 1):\n",
    "                if n_features % i == 0:\n",
    "                    return i, n_features // i\n",
    "        \n",
    "    # Convert features to images\n",
    "    def features_to_image(self, features, width, height):\n",
    "        \"\"\"Converts features given a patient to an image.\n",
    "        \n",
    "        Args:\n",
    "            features (np.array): Features of a patient\n",
    "            width (int): Width of the image\n",
    "            height (int): Height of the image\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Image of the features\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize the features\n",
    "        normalized_features = (features - np.min(features)) / (np.max(features) - np.min(features))\n",
    "\n",
    "        # Calculate the required number of padding zeros\n",
    "        num_features = len(features)\n",
    "        target_size = width * height\n",
    "        padding_zeros = target_size - num_features\n",
    "\n",
    "        # If padding is required, add zeros to the features\n",
    "        if padding_zeros > 0:\n",
    "            normalized_features = np.concatenate((normalized_features, np.zeros(padding_zeros)))\n",
    "\n",
    "        # Reshape the features to the desired width and height\n",
    "        image = normalized_features.reshape(width, height)\n",
    "        return image\n",
    "        \n",
    "    def return_weights(self):\n",
    "        \"\"\"Plots the weights in a histogram.\"\"\"\n",
    "        \n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                weights.append(layer.weight.view(-1))\n",
    "                \n",
    "        plot_weights(torch.cat(weights).detach().numpy())\n",
    "        \n",
    "    def plot_image(self, x):\n",
    "        \"\"\"Plots the feature image of a patient. Converts the image to an actual image.\n",
    "\n",
    "        Args:\n",
    "            x (int): Index of the patient in the dataframe.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.imshow(self.images[x], cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    def score(self):\n",
    "        return f\"Score: {concordance_statistic(self(self.tensor).detach().numpy(), self.df[self.event].to_numpy(), self.df[self.time].to_numpy(), True)}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us test this on the `Rossi` dataset.\n",
    "\n",
    "First, we must scale the data. We can use the `StandardScaler` class from `sklearn` to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rossi_scaled = rossi.copy()\n",
    "\n",
    "# Normalise the data from 0 to 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get the columns that need to be scaled\n",
    "rossi_columns_to_scale = [col for col in rossi_scaled.columns if col not in ['StatusDeath', 'TimeDeath']]\n",
    "\n",
    "# Fit the scaler to the data and transform the selected columns\n",
    "rossi_scaled[rossi_columns_to_scale] = scaler.fit_transform(rossi_scaled[rossi_columns_to_scale])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what kind of input the `CNN` receives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_cnn = CNN(rossi, 'arrest', 'week', 3, 3)\n",
    "rossi_cnn.plot_image(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more data, this should be much more interesting. We can fit the `CNN` model, but it is unlikely to perform extremely well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_cnn.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interetingly, it seems this converged to the local optimal solution very quickly. We can also visualise the risks as usual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_cnn.score()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically the worse model so far but still not that suprisingly bad, especially considering we had to scale the features. Let us also see the weights of this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "rossi_cnn.return_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chronic Heart Failure Dataset\n",
    "\n",
    "First we will measure the c-statistic using the `FWL` model with pre-specified weights.\n",
    "\n",
    "### `FixedWeightLinear` Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create base NN model\n",
    "chf_fwl = FixedWeightLinear(chf_c)\n",
    "\n",
    "# Compute the c-statistic using concordance_statistic\n",
    "print(\"FWL CHF c-statistic:\", get_concordance_statistic_nn(chf, 'StatusDeath', 'TimeDeath', chf_fwl))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using a new dataset, the graph we plot will be different. It is interesting to see how the risk values are distributed. We can see that the risk values are more spread out than the `Rossi` dataset.\n",
    "\n",
    "### `VariableWeightLinear` Model\n",
    "\n",
    "Let us now see the `VWL` model with random weights. It should give the same results as the `FWL` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Apply the VariableWeightLinear model\n",
    "chf_input_size = chf.shape[1] - 2\n",
    "chf_vwl = VariableWeightLinear(chf_input_size) # # Number of input features (excluding censor and time variables)\n",
    "\n",
    "# Initial c-statistic\n",
    "print(\"Initial VWL c-statistic:\", get_concordance_statistic_nn(chf, 'StatusDeath', 'TimeDeath', chf_vwl, False))\n",
    "\n",
    "# Train model\n",
    "# We use the same learning rate as with the bare gradient descent\n",
    "chf_vwl_loss = train_model(chf, 'StatusDeath', 'TimeDeath', chf_vwl, 0.000001, 4000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Final c-statistic\n",
    "chf_vwl_c = get_concordance_statistic_nn(chf, 'StatusDeath', 'TimeDeath', chf_vwl, True)\n",
    "\n",
    "# Print the trained weights\n",
    "print(\"Trained VWL CHF Weights: {0} \\\n",
    "    \\nTrained VWL CHF Loss: {1} \\\n",
    "    \\nTrained VWL CHF c-statistic {2}\"\n",
    "    .format(chf_vwl.weights, chf_vwl_loss, chf_vwl_c))        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different results. Now let's apply the same methods in applying NN models to the `Rossi` dataset upon the `CHF` dataset.\n",
    "\n",
    "### Simple Neural Network Model (`SNN`)\n",
    "\n",
    "We expect that the size of the neurons in the hidden layer will not affect the performance of the model much. Let's see if this is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create a loop to test different architectures (begin with 1 neuron in single hidden layer, increase in steps of 1 until 2 * input_size)\n",
    "chf_snn_losses = []\n",
    "chf_snn_scores = []\n",
    "\n",
    "# This allows us to test different architectures of the NN model\n",
    "# We do not want to add too many neurons because this can lead to overfitting\n",
    "chf_snn_hidden_sizes = [1 + i for i in range(chf_input_size * 2)]\n",
    "\n",
    "# This stores the best model and its score\n",
    "chf_snn_best = [None, 0, 0]\n",
    "\n",
    "# Loop through the different architectures\n",
    "for i in chf_snn_hidden_sizes:\n",
    "    model = SNN(chf_input_size, i)\n",
    "    loss, score = test_nn(chf, 'StatusDeath', 'TimeDeath', model, 0.0001, 1000, False)\n",
    "    if score > chf_snn_best[2]:\n",
    "        chf_snn_best = [model, loss.detach().numpy(), score]\n",
    "    chf_snn_losses.append(loss.detach().numpy())\n",
    "    chf_snn_scores.append(score)\n",
    "\n",
    "# Print the scores of the best architecture\n",
    "print(\"Best SNN CHF: \\\n",
    "    \\n- Architecture: SNN[{0}, {1}, 1] \\\n",
    "    \\n- Loss: {2} \\\n",
    "    \\n- Score: {3}\"\n",
    "    .format(chf_input_size, chf_snn_best[0].hidden_size, chf_snn_best[1], chf_snn_best[2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise the scores of the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Plot the losses\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(chf_snn_hidden_sizes, chf_snn_losses, label='Loss', color='black')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the computed c-statistic scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Plot the c-statistic scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(chf_snn_hidden_sizes, chf_snn_scores, label='C-Statistic', color='black')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('C-Statistic')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the weights of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_snn_best[0].return_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these outperformed standard CPH. Let us investigate further.\n",
    "\n",
    "### General Neural Network Model (`GNN`)\n",
    "\n",
    "Now let us `GNN` models, with similar structures. First we will use `1` more neuron in the first hidden layer than the input layer (i.e. `5`), and square root the number of input neurons in the second hidden layer (i.e. `4`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_gnn = GNN([4, 5, 2, 1])\n",
    "test_nn(chf, 'StatusDeath', 'TimeDeath', chf_gnn, 0.00001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model does not perform as well as the `SNN` model. Let us try using double the number of neurons in the first hidden layer than the input layer (i.e. `8`), and square root the number of input neurons in the second hidden layer (i.e. `2`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_gnn = GNN([4, 8, 2, 1])\n",
    "test_nn(chf, 'StatusDeath', 'TimeDeath', chf_gnn, 0.00001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was better. Let's try increasing the number of neurons in the second hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_architecture = [4, 8, 4, 1]\n",
    "chf_gnn = GNN(chf_architecture)\n",
    "test_nn(chf, 'StatusDeath', 'TimeDeath', chf_gnn, 0.00001, 1000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again a little better. Let's model the weights to make sure we aren't overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_gnn.return_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Class Analysis (`LCA`)\n",
    "\n",
    "Now, it is time to use LCA. Let us first establish the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca = lca(chf, 'StatusDeath', 'TimeDeath', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the data into `2` components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca2 = lca(chf, 'StatusDeath', 'TimeDeath', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca2_arch = lca(chf, 'StatusDeath', 'TimeDeath', 2, chf_architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as with the standard architecture. We can try with `3` classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca3 = lca(chf, 'StatusDeath', 'TimeDeath', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca3_arch = lca(chf, 'StatusDeath', 'TimeDeath', 3, chf_architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally using `4` classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca4 = lca(chf, 'StatusDeath', 'TimeDeath', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_lca4_arch = lca(chf, 'StatusDeath', 'TimeDeath', 4, chf_architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use `5` classes as we have not outperformed results from the standard CPH model, let alone those scored by `Mbotwa et al. (2021)`. Let us try using a CNN model.\n",
    "\n",
    "### Convolutional Neural Network (`CNN`)\n",
    "\n",
    "As mentioned, we must first scale the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_scaled = chf.copy()\n",
    "\n",
    "# Normalise the data from 0 to 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get the columns that need to be scaled\n",
    "columns_to_scale = [col for col in chf_scaled.columns if col not in ['StatusDeath', 'TimeDeath']]\n",
    "\n",
    "# Fit the scaler to the data and transform the selected columns\n",
    "chf_scaled[columns_to_scale] = scaler.fit_transform(chf_scaled[columns_to_scale])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_cnn = CNN(chf_scaled, 'StatusDeath', 'TimeDeath', 2, 2)\n",
    "chf_cnn.plot_image(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the image, we doubt it will be able to capture the relationships between the covariates. Let us see the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_cnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_cnn.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "chf_cnn.return_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not beaten the results scored by `Mbotwa et al. (2021)`. Thus, we will not actively attempt to produce results on the entire dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data\n",
    "\n",
    "We can also visualise the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Calculates number of bins for the histograms by taking the square root of the number of data points (comes out as 50)\n",
    "n_bins = int(np.ceil(np.sqrt(chf.shape[0])))\n",
    "\n",
    "# Plot histogram for ClinicAge\n",
    "# Set figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(chf['ClinicAge'], bins=n_bins, color=\"black\")\n",
    "plt.xlabel('Clinic Age (years)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Plot histogram for ClinicAge\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(chf['Haemoglobin'], bins=n_bins, color=\"black\")\n",
    "plt.xlabel('Haemoglobin (g/dl)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create pie chart for each categorical feature of interest\n",
    "diabetes = chf['Diabetes'].copy()\n",
    "# Replace 0 with 'No Diabetes' and 1 with 'Diabetes'\n",
    "diabetes.replace({0: 'No Diabetes', 1: 'Diabetes'}, inplace=True)\n",
    "diabetes.value_counts().plot(kind='pie', autopct='%1.0f%%', figsize=(3, 3))\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create pie chart for each categorical feature of interest\n",
    "sex = chf['MaleSex=Male'].copy()\n",
    "# Replace 0 with 'Female' and 1 with 'Male'\n",
    "sex.replace({0: 'Female', 1: 'Male'}, inplace=True)\n",
    "sex.value_counts().plot(kind='pie', autopct='%1.0f%%', figsize=(3, 3))\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Calculate standard deviation of each feature\n",
    "std = chf.std()\n",
    "outlier_upper_threshold = chf.mean() + 3 * std\n",
    "outlier_lower_threshold = chf.mean() - 3 * std\n",
    "# Calculate and plot the ratio of number of outliers to number of data in each feature\n",
    "outlier_ratio = ((chf > outlier_upper_threshold).sum() + (chf < outlier_lower_threshold).sum())/chf.shape[0] * 100\n",
    "# Sort x axis by outlier ratio\n",
    "outlier_ratio.sort_values(ascending=False, inplace=True)\n",
    "outlier_ratio.plot(kind='barh', figsize=(4, 4))\n",
    "# Horizontal black bars\n",
    "plt.barh(outlier_ratio.index, outlier_ratio, color='black')\n",
    "plt.xlabel('Ratio of anomalies (%)')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Compute direct correlations of selected features with the target variable\n",
    "correlations_original = chf.corrwith(chf['StatusDeath'])\n",
    "# Sort them according the strength of correlations.\n",
    "correlations_index = correlations_original.abs().sort_values(kind=\"quicksort\", ascending=False).index\n",
    "# Use index to sort correlations\n",
    "correlations = correlations_original[correlations_index]\n",
    "# Display correlation value from original dataframe\n",
    "correlations_df = pd.DataFrame(correlations, index=correlations.index, columns=['Correlation'])\n",
    "# Print correlations\n",
    "display(correlations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\omarc\\OneDrive - University of Leeds\\Year 3\\3931 Individual Project\\Code\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine target variable to the dataframe (new dataframe)\n",
    "correlations_df_with_target = pd.concat([chf, chf['StatusDeath']], axis=1)\n",
    "\n",
    "# Create heatmap of all correlations\n",
    "correlation=correlations_df_with_target.corr().abs()\n",
    "heatmap = sns.heatmap(correlation, vmin=0, vmax=1, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0b10db86501ab32915d4431e7cf763e726bc22f61f5bd74457809bef5b4811e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
