{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">COMP5611M - Introduction to Autograd</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Autograd\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In the notebook *'Datastructures in PyTorch'* we've seen that a network is a mathematical function that can be built of simple blocks: tensors that are much like Numpy arrays and in built functions like sigmoid.  It is meant to be an even gentler introduction than:\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#differentiation-in-autograd,\n",
    "which you should consider next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([81.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([9.], requires_grad=True)\n",
    "\n",
    "f=x**2\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? $f$ is the square of $x$, and its numerical value should be $x^2$, which works out as 9. This is correct, but you also see that f is equiped with a gradient function, which is called *PowBackward0*, which we didn't ask for. Or did we? In x, we specified <code>requires_grad=True</code>, which overrules the default, which is <code>False</code>. \n",
    "\n",
    "We can simply call backward on $f$, which appears to do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we inspect the gradient information of $x$, we find it is correct. If\n",
    "$$\n",
    "f(x) = x^2,\n",
    "$$\n",
    "then \n",
    "$$\n",
    "f^{\\prime}(x) = 2x\n",
    "$$\n",
    "which for $x=9$, works out as $2x =18$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the gradient is state dependent, calling *backward* twice is undefined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This triggers a run time error.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    f.backward()\n",
    "except RuntimeError:\n",
    "    print('This triggers a run time error.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A slightly more complex example:\n",
    "Consider \n",
    "$$\n",
    "f(x,y) = \\sqrt{x^2 + y^2}\n",
    "$$\n",
    "We want to calculate\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} \\mid_{(3,2)}\n",
    "$$\n",
    "\n",
    "**Exercise**: Verify that:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\frac{x}{\\sqrt{x^2 + y^2}}\n",
    "$$\n",
    "so \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} \\mid_{(3,2)} = \\frac{3}{\\sqrt{9+4}} = \\frac{3}{\\sqrt{13}} \\approx 0.83209\n",
    "$$\n",
    "Also,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} \\mid_{(3,2)} = \\frac{2}{\\sqrt{9+4}} = \\frac{2}{\\sqrt{13}} \\approx 0.5547\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8321])\n",
      "tensor([0.5547])\n"
     ]
    }
   ],
   "source": [
    "xn = torch.tensor([3.],requires_grad=True)\n",
    "yn = torch.tensor([2.],requires_grad=True)\n",
    "\n",
    "fn = torch.sqrt(xn**2+yn**2)\n",
    "\n",
    "fn.backward()\n",
    "print(xn.grad)\n",
    "print(yn.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Gradient\n",
    "\n",
    "We know that if we have a perceptron that:\n",
    "$$\n",
    "o(x) = \\sigma( \\boldsymbol{w}^T \\boldsymbol{x})\n",
    "$$\n",
    "Here, $\\sigma$ is the sigmoid:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "In the main text, we show that:\n",
    "$$\n",
    "\\frac{\\partial o}{ \\partial \\boldsymbol{w}} = o(1-o)\\boldsymbol{\\boldsymbol{x}}\n",
    "$$\n",
    "\n",
    "Let's pick:\n",
    "$$\n",
    "\\boldsymbol{w}^T = (1,1,1)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\boldsymbol{x}^T = (1,2,3)\n",
    "$$\n",
    "We find that \n",
    "$$\n",
    "o = \\sigma( 1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 3 ) = \\frac{1}{1 + e^{-6}} = 0.99752\n",
    "$$\n",
    "For $\\boldsymbol{w}^T$ we find:\n",
    "$$\n",
    "\\boldsymbol{w}^T = o(1-o)\\boldsymbol{x}^T = (0.00246651, 0.00493302, 0.00739953)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.00246647, 0.00493293, 0.00739940])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "w=torch.tensor([1.,1.,1.],requires_grad = True)\n",
    "x=torch.tensor([1.,2.,3.])\n",
    "o=torch.sigmoid(x@w)\n",
    "o.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: The results is close enough to inspire confidence that the computation is by and large correct, yet small deviations are visible. Explain whether this is cause for concern. In your answer consider what the machine representation for these tensor are (how would you find out?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does PyTorch achieve this?\n",
    "\n",
    "Any mathematical expression is decomposed as a graph. Every mathematical operator has a method called *forward* which implements its actual implementation. They also a *backward* method, which implements the derivative. We have already seen the *forward* method when we built our own net.  In general, it is not necessary to implement the *backward* method for your own objects because they can be inferred, as long as you use pytorch objects to build your forward *computational graph*\n",
    "\n",
    "**Exercise**: Define your own sigmoid function instead of torch.sigmoid. Use it in the implementation of the Net class. Run the rest of the notebook to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.weights = torch.tensor([1.,1.,1.],requires_grad=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.h=torch.sigmoid(x@self.weights)\n",
    "        return self.h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *forward* method we can apply the network directly on pattern $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.99752742, grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "net=Net()\n",
    "x=torch.tensor([1.,2.,3.])\n",
    "o=net(x)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to print the gradient of the weights, we will find it's not yet available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net.weights.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling backward on the calculation result, which is still inside the net object, we have the correct gradient information on the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.00246647, 0.00493293, 0.00739940])\n"
     ]
    }
   ],
   "source": [
    "net.h.backward()\n",
    "print(net.weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of a loss function\n",
    "\n",
    "We're usually interestin the gradient with respect to a loss function, not in the gradient vector of the network. The gradient of the loss function tells us in which direction our weights need to change.\n",
    "\n",
    "Consider the MSE loss function:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2}(d - o)^2,\n",
    "$$\n",
    "where $d$ is the desired network output, and $o$, the actual network output. In our previous example, we entered\n",
    "the pattern $\\boldsymbol{x}^T = (1, 2, 3)$ in the network. The actual output already has been calculated to be\n",
    "0.9975.\n",
    "\n",
    "For this input point, what is the gradient? This depends on how far the output is removed from the desired output.\n",
    "Let us assume that \n",
    "$$\n",
    "d=0.9\n",
    "$$\n",
    "\n",
    "Working out the gradient algebraically we find:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{w}} = (o-d) \\frac{\\partial o}{\\partial \\boldsymbol{w}} =\n",
    "(o-d)o(1-o)\\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "All these quantities are known for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.00024055, 0.00048110, 0.00072164], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d=0.9\n",
    "print((o-d)*o*(1-o)*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that automatic differentiation can produce the same result without need for deriving\n",
    "the gradient algebraically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.99752742, grad_fn=<SigmoidBackward>)\n",
      "tensor(0.00475580, grad_fn=<MulBackward0>)\n",
      "tensor([0.00024055, 0.00048110, 0.00072164])\n"
     ]
    }
   ],
   "source": [
    "d=0.9\n",
    "# A new net\n",
    "mod=Net()\n",
    "# Produce an output, the naming of the variables is more in line with PyTorch conventions\n",
    "pred=mod(x)\n",
    "print(pred)\n",
    "# Now we construct a loss function, which we can evaluate for this output\n",
    "loss = 0.5*(pred-d)**2\n",
    "print(loss)\n",
    "# Now calculate the gradient - of the loss function!! - with respect to the weights\n",
    "loss.backward()\n",
    "print(mod.weights.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs\n",
    "\n",
    "### How does autograd do it (and why do I almost don't need to do anything?)\n",
    "\n",
    "Consider the expression:\n",
    "$$\n",
    "\\cos(\\frac{1}{x+y})\n",
    "$$\n",
    "and consider differentiation with respect to $x$ in the point $(1,2)$.\n",
    "We can imagine the computation as a directed acyclical graph (DAG), where each node has a well defined role.\n",
    "\n",
    "\n",
    "   ![computational graph](forwardbackward.png)\n",
    "   \n",
    "To perform the differentiation we need to apply the chain rule:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} \\cos( \\frac{1}{x+y}) = -\\sin( \\frac{1}{x+y}) \\cdot -(x+y)^{-2} \\cdot 1\n",
    "$$\n",
    "\n",
    "We see that the chain rule walks the DAG in reverse order, and can do so if at each node it knows what the derivative is of the corresponding forward node, and inserts numerical values that have been calculated in the forward pass.\n",
    "\n",
    "**The chain rule is backpropagation**\n",
    "\n",
    "The feedforward network discussed in the main text is just a particularly simple DAG.\n",
    "\n",
    "Most mathematical functions in Pytorch have a forward and a backward pass. If you create complex functions\n",
    "built from PyTorch objects, the backward pass is already available and you do not have to implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
